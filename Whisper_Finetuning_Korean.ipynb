{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uhalpern/ICS-435-Final-Project/blob/main/Whisper_Finetuning_Korean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ICS 435 Final - Fine-tuning Whisper From OpenAI**"
      ],
      "metadata": {
        "id": "djdgxcsCcS6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup code for the model and datasets was adapted from this [blog post](https://huggingface.co/blog/fine-tune-whisper) on huggingface."
      ],
      "metadata": {
        "id": "gqK4QM3qcjl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparing Environment**"
      ],
      "metadata": {
        "id": "miESp4evdh-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run if you would like to work in Google Drive Directory\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J8DNPo3lp9HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/ICS-435-Final-Project"
      ],
      "metadata": {
        "id": "CKeJV5Pyp0fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio evaluate\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ZWMu9DR9d20x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Dataset Through Hugging Face**"
      ],
      "metadata": {
        "id": "zgb-iWmuDmAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "IHO3J9WpDWaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the dataset\n",
        "common_voice = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"ko\", split=\"train+validation+test\", use_auth_token=True)\n",
        "\n",
        "# Remove uneeded columns\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"])\n",
        "\n",
        "# Randomly permute the dataset\n",
        "common_voice = common_voice.shuffle(seed=42)\n",
        "\n",
        "# Calculate the sizes for train, validation, and test sets\n",
        "total_size = len(common_voice)\n",
        "train_size = int(0.6 * total_size)\n",
        "val_size = int(0.25 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Adjust sizes to include all examples\n",
        "if train_size + val_size + test_size < total_size:\n",
        "    test_size += total_size - (train_size + val_size + test_size)\n",
        "elif train_size + val_size + test_size > total_size:\n",
        "    # Adjust train size to avoid exceeding total size\n",
        "    train_size -= (train_size + val_size + test_size) - total_size\n",
        "\n",
        "# Create DatasetDict with train, validation, and test splits\n",
        "splits = {\n",
        "    \"train\": common_voice.select(range(train_size)),\n",
        "    \"validation\": common_voice.select(range(train_size, train_size + val_size)),\n",
        "    \"test\": common_voice.select(range(train_size + val_size, total_size))\n",
        "}\n",
        "\n",
        "common_voice = DatasetDict(splits)"
      ],
      "metadata": {
        "id": "uzBCbNIqExcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define feature extractor that pads or truncates audio inputs into 30 second segments. Then, the audio input is converted into log-Mel spectrogram form."
      ],
      "metadata": {
        "id": "ysCbxsYLeF6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "aiLOneOYNu9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Whisper tokenizer that maps token ids to corresponding text string."
      ],
      "metadata": {
        "id": "XWKsUBPygWzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Korean\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "1MtDpTAHOUS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Korean\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "Wxi79ya2O4vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downsample audio to 16kHz as expected by the Whisper model"
      ],
      "metadata": {
        "id": "2QpbJMdkPNo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "sNGFKOpDPFXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to prepare the dataset\n",
        "\n",
        "\n",
        "1.   Resample audio to 16000 kHz\n",
        "2.   Compute log-Mel spectrogram on input features in order to separate individual frequencies\n",
        "3. Assign labels to transcriptions"
      ],
      "metadata": {
        "id": "coeEdl3KPdzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "MinUe1qFPWxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map the pre-processing function to the entire dataset"
      ],
      "metadata": {
        "id": "_SSiocIcPzph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "iUhZD3S2Pv7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training and Evaluation**"
      ],
      "metadata": {
        "id": "qxEot4dDZPg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "fpq-mvn3ZXN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define language and task for model\n",
        "model.generation_config.language = \"korean\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None"
      ],
      "metadata": {
        "id": "mKUCFPiJaSz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define data collator that will convert the input_features to batched PyTorch sensors. The labels will also be padded in order to match the padding given to the input_features. Padding tokens are represented by -100 so that it does not affect the loss."
      ],
      "metadata": {
        "id": "Q3X8nHypaaYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "VOB0nJM8aYB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "SfvtT_jod76F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up evaluation metrics"
      ],
      "metadata": {
        "id": "C-JRk60QiIjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "0ypu0DpwiK4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to return WER metric from model predictions to handle padded tokens. Decode the predicted and label ids to strings and compute the WER between predicted strings and labels."
      ],
      "metadata": {
        "id": "q7uJsby6iSDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "u2ByN-0Simp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists with three options for each hyperparameter\n",
        "learning_rate_options = [1e-6, 1e-5, 1e-4]\n",
        "warmup_steps_options = [10, 20, 30]\n",
        "batch_gradient_tuples = [(32, 4), (64, 4), (128, 2)]\n",
        "weight_decay_options = [0, 0.2, 0.3]\n",
        "num_epochs_options = [5, 10, 15]"
      ],
      "metadata": {
        "id": "NEYRVFqwE-l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "random.seed(42)\n",
        "training_args_list = []\n",
        "\n",
        "# Perform random search with 5 passes\n",
        "for i in range(5):\n",
        "    # Randomly select hyperparameter values\n",
        "    learning_rate = random.choice(learning_rate_options)\n",
        "    warmup_steps = random.choice(warmup_steps_options)\n",
        "    batch_size, gradient_accumulation_steps = random.choice(batch_gradient_tuples)\n",
        "    weight_decay = random.choice(weight_decay_options)\n",
        "    num_epochs = random.choice(num_epochs_options)\n",
        "\n",
        "    total_examples = len(common_voice[\"train\"])\n",
        "\n",
        "    # Calculate steps per epoch based on batch size\n",
        "    steps_per_epoch = total_examples // batch_size\n",
        "\n",
        "    # Calculate total training steps\n",
        "    total_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    # Create model and evaluation checkpoints at half the total steps\n",
        "    save_steps = total_steps // 2\n",
        "    eval_steps = total_steps // 2\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=f\"./whisper-small-ko_pass_{i+1}\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=total_steps,\n",
        "        gradient_checkpointing=True,\n",
        "        fp16=False,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        per_device_eval_batch_size=4,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=225,\n",
        "        save_steps=save_steps,\n",
        "        eval_steps=eval_steps,\n",
        "        logging_steps=25,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"wer\",\n",
        "        greater_is_better=False,\n",
        "        push_to_hub=True,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    training_args_list.append(training_args)\n",
        "\n",
        "    print(f\"Pass {i+1}:\")\n",
        "    print(f\"Learning Rate: {learning_rate}\")\n",
        "    print(f\"Warmup Steps: {warmup_steps}\")\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
        "    print(f\"Weight Decay: {weight_decay}\")\n",
        "    print(f\"Num Epochs: {num_epochs}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "bFwm6sp-F1Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "2qWea0QhOiuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Iterate over training arguments list and train the models\n",
        "for i, training_args in enumerate(training_args_list):\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        args=training_args,\n",
        "        model=model,\n",
        "        train_dataset=common_voice[\"train\"],\n",
        "        eval_dataset=common_voice[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=processor.feature_extractor,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training model {i+1}...\")\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "Qa76iZ9kHxM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate The Base Model on The Held Out Test Set"
      ],
      "metadata": {
        "id": "U1PYtGJqhSb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "u0fQyagCin5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "2Le_dTA5JcAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Best Finetuned Model and Evaluate on Held Out Test Set"
      ],
      "metadata": {
        "id": "x5X_9molhgNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"uhalpern/whisper-small-ko_pass_2\")"
      ],
      "metadata": {
        "id": "Q8bmbODQOqQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "9pjC9LzNg0wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "UKQG-Sc3g2l3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}